import json
import time
import csv
import os
import re
import glob
import collections
import sys
import shutil
import traceback
import codecs
import pprint
import numpy as np
from scipy.sparse import csr_matrix
from tqdm import tqdm
import random
import openpyxl

sys.path.append('C:\\Users\\Tsuyoshi Yamashita\\source\\repos\\apiread\\apiread\\libsvm-master\\python\\')
sys.path.append('C:\\Users\\Tsuyoshi Yamashita\\source\\repos\\apiread\\apiread\\libsvm-master\\tools\\')

from word2vec import Word2Vec, Sent2Vec, LineSentence
from itertools import zip_longest
from svm import *
from svmutil import *
from commonutil import *
from grid import *

import class_summary

from sklearn.svm import SVC
from sklearn.metrics import make_scorer
from sklearn.metrics import accuracy_score, recall_score, precision_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPClassifier

from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler, NearMiss
from imblearn.pipeline import make_pipeline

from gensim import models
from gensim.models.doc2vec import Doc2Vec, LabeledSentence

api_type = list()
api_list = list()
summary_list = list()
summary_count_dict = dict()

api_type_count = 0
count_sample = 0
count_all = 0
count_able = 0
count = 0
type_summary = 0

CROSS_VAL_NUM = 10	# 分割交差検証の分割数

file_sample_name = 'sample_name.txt'
file_summary_list = 'summary.txt'			# 機能の種類を保存するファイルのパス指定
file_summary_count = 'summary_count.csv'	# 機能の出現回数を保存するファイルのパス指定
file_path = 'api_only2.txt'					# APIコール列を保存するファイルのパス指定

#================================================================================================

with open(file_summary_list, 'r') as f:
	l = list()
	l = [s.strip('\n') for s in f.readlines()]
		
	for s in l:
		sum = class_summary.class_summary()
		sum.name = s
		summary_list.append(sum)

"""
for sl in summary_list:
	sl_path = '.\\' + sl.name + '\\'

	if os.path.exists(sl_path + 'inc_name.txt'):
		os.remove(sl_path + 'inc_name.txt')
	elif not os.path.exists(sl_path + 'inc_name.txt'):
		with open(sl_path + 'inc_name.txt', mode='x') as f:
			f.write('')

	if os.path.exists(sl_path + 'inc_vec.txt'):
		os.remove(sl_path + 'inc_vec.txt')
	elif not os.path.exists(sl_path + 'inc_vec.txt'):
		with open(sl_path + 'inc_vec.txt', mode='x') as f:
			f.write('')

	if os.path.exists(sl_path + 'not_inc_name.txt'):
		os.remove(sl_path + 'not_inc_name.txt')
	elif not os.path.exists(sl_path + 'not_inc_name.txt'):
		with open(sl_path + 'not_inc_name.txt', mode='x') as f:
			f.write('')

	if os.path.exists(sl_path + 'not_inc_vec.txt'):
		os.remove(sl_path + 'not_inc_vec.txt')
	elif not os.path.exists(sl_path + 'not_inc_vec.txt'):
		with open(sl_path + 'not_inc_vec.txt', mode='x') as f:
			f.write('')
"""

# [[[APIコール列の作成]]]
print('apicall.txt を更新しますか。y/n ')
if input() is 'y':
	#shutil.rmtree(file_path)

# 2016～2017年のDataset
	for year in range(2016, 2018):

		# ファイル読み込み、辞書の作成
		file_list = glob.glob('FFRI_Dataset_{}/*.json'.format(year))
		for i, name in enumerate(file_list):
			json_open = open(name, 'r')
			print('File Name :', name)

			count_all += 1

			try:
				json_load = json.load(json_open)
			except json.JSONDecodeError:
				json_open.close()
				continue

			# 'behaivior' 及び 'processes' が存在するかの確認
			try:
				processes_range = range(len(json_load['behavior']['processes']))
			except KeyError:
				print('↑KeyError---Behavior,Processes')
				json_open.close()
				continue

			# apiの確認
			is_api = False
			is_key = True
			for processes_i in processes_range:
				try:
					calls_range = range(len(json_load['behavior']['processes'][processes_i]['calls']))
				except KeyError:
					is_key = False
					break

				for calls_i in calls_range:
					if(json_load['behavior']['processes'][processes_i]['calls'][calls_i]['api']):
						is_api = True
						count_able += 1
						break

				if is_api == True:
					break

			if is_api == False:
				print('↑KeyError---API')
				json_open.close()
				continue			


			# 'summary' の有無を確認
			try:
				summary_name = json_load['behavior']['summary']
			except KeyError:
				print('↑KeyError---Summary')
				is_key = False
				continue

			# 'sha1' の有無を確認
			try:
				sample_name = json_load['virustotal']['sha1'].strip('"')
			except KeyError:
				print('↑KeyError---Sha1')
				is_key = False
				continue

			count_sample += 1
			"""
			# 'summary' から機能をリストに加える
			for s in json_load['behavior']['summary']:
				if not s in summary_list:
					summary_list.append(s)
					"""
			# APIコール列を書き込む
			if not os.path.isfile(file_path):
				#with codecs.open(file_path, 'xb', 'cp932', 'ignore') as f:
				with open(file_path, mode='x') as f:
					try:
						for v in json_load['behavior']['processes'][0]['calls']:
							f.write(v['api'].strip('"') + ' ')
							"""
							for w in v['arguments'].items():
								f.write(w[0].strip('"') + ' ')
								f.write(str(w[1]))
								f.write(' ')
								"""
						f.write('\n')

					except:
						traceback.print_exc()
						print('Behavior Error')
						if input() is 'y':
							continue
						elif input() is 'n':
							break

			else:
				#with codecs.open(file_path, 'ab', 'cp932', 'ignore') as f:
				with open(file_path, mode='a') as f:
					try:
						for v in json_load['behavior']['processes'][0]['calls']: 
							f.write(v['api'].strip('"') + ' ')
							"""
							for w in v['arguments'].items():
								f.write(w[0].strip('"') + ' ')
								f.write(str(w[1]))
								f.write(' ')
								"""
						f.write('\n')

					except:
						traceback.print_exc()
						print('Behavior Error')
						if input() is 'y':
							continue
						elif input() is 'n':
							break

			# 検体名をファイルに書き込む
			with open(file_sample_name, mode='a') as f:
				f.write(sample_name + '\n')

	print(count_sample)
			
#==================================================================================================					

# [[[機能をファイルに書き込む]]]

print('summary.txt を更新しますか。y/n ')
if input() is 'y':
	#shutil.rmtree(file_summary_list)

	for year in range(2016, 2018):

		# ファイル読み込み、辞書の作成
		file_list = glob.glob('FFRI_Dataset_{}/*.json'.format(year))
		for i, name in enumerate(file_list):
			json_open = open(name, 'r')
			print('File Name :', name)

			try:
				json_load = json.load(json_open)
			except json.JSONDecodeError:
				json_open.close()
				continue

			# 'behaivior' 及び 'processes' が存在するかの確認
			try:
				processes_range = range(len(json_load['behavior']['processes']))
			except KeyError:
				print('↑KeyError---Behavior,Processes')
				json_open.close()
				continue

			# apiの確認
			is_api = False
			is_key = True
			for processes_i in processes_range:
				try:
					calls_range = range(len(json_load['behavior']['processes'][processes_i]['calls']))
				except KeyError:
					is_key = False
					break

				for calls_i in calls_range:
					if(json_load['behavior']['processes'][processes_i]['calls'][calls_i]['api']):
						is_api = True
						break

				if is_api == True:
					break

			if is_api == False:
				print('↑KeyError---API')
				json_open.close()
				continue			


			# 'summary' の有無を確認
			try:
				summary_name = json_load['behavior']['summary']
			except KeyError:
				print('↑KeyError---Summary')
				is_key = False
				continue

			"""
			# 'summary' から機能をファイルに書き込む
			for s in json_load['behavior']['summary']:
				if not s in summary_list:
					summary_list.append(s)
					with open(file_summary_list, mode='a') as f:
						f.write(s.strip('"') + '\n')
			"""
			try:
				for v in json_load['behavior']['processes'][0]['calls'][0]['api']:
					if not v in api_type:
						api_type.append(v)
						api_type_count += 1

			except:
				traceback.print_exc()
				print('Behavior Error')
				if input() is 'y':
					continue
				elif input() is 'n':
					break

	print(api_type_count)

						

#================================================================================================

# [[[機能ごとに検体数を計測]]]
print('機能の検体数を計測しますか。y/n ')
if input() is 'y':
	# 全機能をリストに保存
	with open(file_summary_list) as f:
		l = list()
		l = [s.strip('\n') for s in f.readlines()]

		value = list()
		summary_count = list()
		for k, v in zip_longest(l, value, fillvalue = 0):
			summary_count_dict.setdefault(k, v)
			summary_count.append(0)
			type_summary += 1
	
	for year in range(2016, 2018):

		# ファイル読み込み、辞書の作成
		file_list = glob.glob('FFRI_Dataset_{}/*.json'.format(year))
		for i, name in enumerate(file_list):
			json_open = open(name, 'r')
			print('File Name :', name)

			try:
				json_load = json.load(json_open)
			except json.JSONDecodeError:
				print('Error---File')
				json_open.close()
				continue

			# 'behaivior' 及び 'processes' が存在するかの確認
			try:
				processes_range = range(len(json_load['behavior']['processes']))
			except KeyError:
				print('↑KeyError---Behavior,Processes')
				json_open.close()
				continue

			# apiの確認
			is_api = False
			is_key = True
			
			for processes_i in processes_range:
				try:
					calls_range = range(len(json_load['behavior']['processes'][processes_i]['calls']))
				except KeyError:
					is_key = False
					break

				for calls_i in calls_range:
					if(json_load['behavior']['processes'][processes_i]['calls'][calls_i]['api']):
						is_api = True
						break

				if is_api == True:
					break

				
			if is_api == False:
				print('↑KeyError---API')
				json_open.close()
				continue			


			# 'summary' の有無を確認
			is_summary = False
			try:
				summary_range = range(len(json_load['behavior']['summary']))
			except KeyError:
				print('↑KeyError---Summary')
				is_key = False
				continue

			for summary_i in summary_range:
				if(json_load['behavior']['summary']):
					is_summary = True
					break

			if is_summary == False:
				print('↑KeyError---Summary')
				json_open.close()
				continue

			count_sample += 1
			
			# 機能をカウントする
			for v in json_load['behavior']['summary']:
				i = 0
				for s in summary_count_dict.keys():
					if v == s:
						summary_count[i] += 1
					i += 1
					
	# カウントした数を代入
	for i in range(0, type_summary):
		summary_count_dict[l[i]] = summary_count[i]
		
	with open(file_summary_count, mode='w') as f:
		writer = csv.DictWriter(f, l)
		writer.writeheader()
		writer.writerow(summary_count_dict)

	print(count_sample)
	pprint.pprint(summary_count_dict)

#================================================================================================

# [[[特徴ベクトルの作成]]]
print('特徴ベクトルを作成しますか。y/n ')
if input() is 'y':
	# 特徴ベクトルの更新
	print('　特徴ベクトルを更新しますか。y/n')
	if input() is 'y':
		"""
		input_file = 'api_only2.txt'
		model = Word2Vec(LineSentence(input_file), size=100, window=5, sg=1, min_count=5, workers=8)
		model.save(input_file + '.model')
		model.save_word2vec_format(input_file + '.vec')

		sent_file = 'api_only2.txt'
		model = Sent2Vec(LineSentence(sent_file), model_file=input_file + '.model')
		model.save_sent2vec_format(sent_file + '.vec')
		"""
		input_file = 'api_only_doc.txt'
		model = models.Doc2Vec(LabeledSentence(input_file), size=100, window=5, dm=1, min_count=5, workers=8)
		model.save(input_file + '.model')

	# 'sentX'を検体名に置換
	vec_file_name = 'api_only2.txt.vec'

	# 特徴ベクトルのバックアップ，sent_Xを検体名に置換
	print('　バックアップをとりますか。y/n')
	if input() is 'y':
		# バックアップの作成
		back_file = vec_file_name + '.bak'
		shutil.copy(vec_file_name, back_file)

		# 検体のリストを読み込み
		with open(file_sample_name) as file:
			sample_list = file.read().splitlines()
			print(file_sample_name + ' loaded')
		
		# vecファイルの読み込み
		with open(vec_file_name, 'r') as vfile:
			vec_lines = vfile.read()
			print(vec_file_name + ' loaded')

		sent_count = 0
		for name in sample_list:
			sent = 'sent_' + str(sent_count)
			if sent in vec_lines:
				vec_lines = vec_lines.replace(sent, name, 1)
				print('sent_' + str(sent_count) + 'replaced with' + name)

			sent_count += 1

		with open(vec_file_name, 'w') as file:
			file.write(vec_lines)

	# summary_listの更新
	print('　summary_listを更新しますか。y/n')
	if input() is 'y':
		#start = time.time()

		# 検体のリストを読み込み
		with open(file_sample_name) as file:
			sample_list = file.read().splitlines()
			print(file_sample_name + ' loaded (number of samples : ' + str(len(sample_list)) + ')')
			print('(number of summary_list : ' + str(len(summary_list)) + ')')

		# vecファイルの読み込み
		with open(vec_file_name, 'r') as vfile:
			vec_list = vfile.read().splitlines()
			print(vec_file_name + ' loaded')

		for year in range(2016, 2018):

			# ファイル読み込み、辞書の作成
			file_list = glob.glob('FFRI_Dataset_{}/*.json'.format(year))
			for i, name in enumerate(file_list):
				json_open = open(name, 'r')
				print('File Name :', name)

				try:
					json_load = json.load(json_open)
				except json.JSONDecodeError:
					print('Error---File')
					json_open.close()
					continue

				# 'behaivior' 及び 'processes' が存在するかの確認
				try:
					processes_range = range(len(json_load['behavior']['processes']))
				except KeyError:
					print('↑KeyError---Behavior,Processes')
					json_open.close()
					continue

				# apiの確認
				is_api = False
				is_key = True
			
				for processes_i in processes_range:
					try:
						calls_range = range(len(json_load['behavior']['processes'][processes_i]['calls']))
					except KeyError:
						is_key = False
						break

					for calls_i in calls_range:
						if(json_load['behavior']['processes'][processes_i]['calls'][calls_i]['api']):
							is_api = True
							break

					if is_api == True:
						break

				
				if is_api == False:
					print('↑KeyError---API')
					json_open.close()
					continue			


				# 'summary' の有無を確認
				is_summary = False
				try:
					summary_range = range(len(json_load['behavior']['summary']))
				except KeyError:
					print('↑KeyError---Summary')
					is_key = False
					continue

				for summary_i in summary_range:
					if(json_load['behavior']['summary']):
						is_summary = True
						break

				if is_summary == False:
					print('↑KeyError---Summary')
					json_open.close()
					continue

				count_sample += 1

				for sl in summary_list:
					sl_path = '.\\' + sl.name + '\\'
					flag = 0
					for v in json_load['behavior']['summary']:
						if v == sl.name:
							with open(sl_path + 'inc_name_doc.txt', mode='a') as f:
								f.write(json_load['virustotal']['sha1'].strip('"') + '\n')
							with open(sl_path + 'inc_vec_doc.txt', mode='a') as f:
								f.write(vec_list[i+1] + '\n')
							flag = 1
						
					if flag == 0:
						with open(sl_path + 'not_inc_name_doc.txt', mode='a') as f:
							f.write(json_load['virustotal']['sha1'].strip('"') + '\n')
						with open(sl_path + 'not_inc_vec_doc.txt', mode='a') as f:
							f.write(vec_list[i+1] + '\n')

		"""
		t = time.time() - start
		hour = t / 2400
		minute = (t - hour*2400) / 60
		second = (t - hour*2400 - minute*60)
		print(hour + '時間' + minute + '分' + second + '秒')
		"""
							

#=================================================================================================

# [[[機械学習]]]
print('機械学習を行いますか。y/n ')
if input() is 'y':
	#start = time.time()
	
	# Excelファイルの書き込み準備
	wb = openpyxl.load_workbook('SVMResult.xlsx')
	sheet = wb['Sheet1']
	sheet_count = 3

	# summary_listの更新
	for sum in summary_list:
		sum_path = '.\\' + sum.name + '\\'
		file_list = glob.glob(sum_path + '*.txt')

		for file_name in file_list:
			# 機能を含む検体名のデータ読み込み
			if file_name == sum_path + 'inc_name.txt':
				with open(file_name, 'r') as file:
					inc_name = file.read().splitlines()	
					for la in inc_name:
						sum.include.append(la)
					sum.include_number = len(inc_name)
			# 機能を含まない検体名のデータ読み込み
			if file_name == sum_path + 'not_inc_name.txt':
				with open(file_name, 'r') as file:
					not_inc_name = file.read().splitlines()	
					for la in not_inc_name:
						sum.non_include.append(la)
					sum.non_include_number = len(not_inc_name)
			# 機能を含む検体のParagraph Vectorのデータ読み込み
			if file_name == sum_path + 'inc_vec.txt':
				with open(file_name, 'r') as file:
					inc_vec = file.read().splitlines()	
					for la in inc_vec:
						lb = la.split()
						lb.pop(0)
						for lc in lb:
							sum.par_vec.append(lc)
			# 機能を含まない検体のParagraph Vectorのデータ読み込み
			if file_name == sum_path + 'not_inc_vec.txt':
				with open(file_name, 'r') as file:
					not_inc_vec = file.read().splitlines()	
					for la in not_inc_vec:
						lb = la.split()
						lb.pop(0)
						for lc in lb:
							sum.non_par_vec.append(lc)

		print('Updated ' + sum.name)

	results = {}

	# 機能ごとに検体数を10分割にする
	for sl in summary_list:
		"""
		sampling_num_inc = sl.include_number - sl.include_number % CROSS_VAL_NUM
		sampling_num_non_inc = sl.non_include_number - sl.non_include_number % CROSS_VAL_NUM

		print(sampling_num_inc, sampling_num_non_inc)
		"""
		
		#rs = random.randint(0,100)

		# 機能を保有する検体のサンプリング
		"""
		random.seed(rs)
		index_list = random.sample(range(0, sl.include_number), sampling_num_inc)
		"""
		par_vec_list = []
		label_list = []
		for vec in sl.par_vec:
			par_vec_list.append(vec)
			label_list.append(1)

		# 機能を保有しない検体のサンプリング
		"""
		random.seed(rs)
		index_list = random.sample(range(0, sl.non_include_number), sampling_num_non_inc)
		"""
		for non_vec in sl.non_par_vec:
			par_vec_list.append(non_vec)
			label_list.append(0)

		"""
		random.seed(rs)
		random.shuffle(par_vec_list)

		random.seed(rs)
		random.shuffle(label_list)

		print(par_vec_list)

		for pvl in par_vec_list:
			sl.svm_data.append(float(pvl))

		for lali in label_list:
			sl.svm_label.append(lali)
		"""

		X = np.array(par_vec_list, dtype=float)
		y = np.array(label_list, dtype=int)

		print("X,Y Updated")
		
		# ESTIMATOR = make_pipeline(StandardScaler(), SVC())
		
		# ESTIMATOR = make_pipeline(StandardScaler(), RandomUnderSampler(random_state=0), SVC())
		
		ESTIMATOR = make_pipeline(SVC())
		
		# ESTIMATOR = make_pipeline(StandardScaler(), ClusterCentroids(random_state=0), SVC())

		# ESTIMATOR = make_pipeline(StandardScaler(), NearMiss(version=1), SVC())
		
		PARAM_GRID = [{'svc__C': [1], 'svc__kernel': ['rbf'], 'svc__gamma':['scale']}]
		
		CV = StratifiedKFold(n_splits=10)
		
		SCORING = {'acc_score':make_scorer(accuracy_score), 'recall_score':make_scorer(recall_score), 'precision_score':make_scorer(precision_score)}
		
		clf = GridSearchCV(
			estimator=ESTIMATOR,
			param_grid=PARAM_GRID,
			scoring=SCORING,
			n_jobs=-1,
			refit=False,
			cv=CV
			)
		print("Learning")

		clf.fit(X, y)
		
		d = clf.cv_results_
		
		results[func] = {\
			'num':num, \
			'mean_fit_time':d['mean_fit_time'][0], \
			'std_fit_time':d['std_fit_time'][0], \
			'mean_score_time':d['mean_score_time'][0], \
			'std_score_time':d['std_score_time'][0], \
			'params':d['params'][0], \
			'mean_test_acc_score':d['mean_test_acc_score'][0], \
			'std_test_acc_score':d['std_test_acc_score'][0], \
			'mean_test_recall_score':d['mean_test_recall_score'][0], \
			'std_test_recall_score':d['std_test_recall_score'][0], \
			'mean_test_precision_score':d['mean_test_precision_score'][0], \
			'std_test_precision_score':d['std_test_precision_score'][0]\
			}
		
		pprint.pprint(results[func], width=1)
	
	l = []

	for key, value in results.items():
		
		l.append([\
			key, \
			value['num'], \
			value['mean_fit_time'], \
			value['std_fit_time'], \
			value['mean_score_time'], \
			value['std_score_time'], \
			value['params'], \
			value['mean_test_acc_score'], \
			value['std_test_acc_score'], \
			value['mean_test_recall_score'], \
			value['std_test_recall_score'],	\
			value['mean_test_precision_score'], \
			value['std_test_precision_score']\
			])

	df = pd.DataFrame(l)
	df.columns = [\
		'func', \
		'num', \
		'mean_fit_time', \
		'std_fit_time', \
		'mean_score_time', \
		'std_score_time', \
		'params', \
		'mean_test_acc_score', \
		'std_test_acc_score', \
		'mean_test_recall_score', \
		'std_test_recall_score', \
		'mean_test_precision_score', \
		'std_test_precision_score'\
		]
	df.to_csv('Result.csv')

	"""
	# SVM学習の開始
		sum_path = '.\\' + sl.name + '\\'
		train_path = sum_path + 'SVMData.train'
		if os.path.exists(train_path):
			os.remove(train_path)
		
		# スケーリング
		csr = csr_matrix(sl.svm_data)
		param = csr_find_scale_param(csr)
		csr = csr_scale(csr, param)
		svm_data = csr.toarray().tolist()

		with open(train_path, 'a') as data_file:
			for i, l in enumerate(svm_data):
				data_file.write(str(sl.svm_label[i]))
				for index, value in enumerate(l):
					data_file.write(' ' + str(index+1) + ':' + str(value))
				data_file.write('\n')
					
		print('↓↓↓' + sl.name + '↓↓↓')
		grid_result = find_parameters(train_path, ['-v', str(CROSS_VAL_NUM), '-out', train_path + '.out'])
		print('Result -> rate:{0}, cost:{1}, gamma:{2}'.format(str(grid_result[0]), str(grid_result[1]['c']), str(grid_result[1]['g'])))
		
		average = (int(grid_result[0] * 100)) / 100.0	# 小数点第3位以下を切り捨て

		# Excelファイルに書き込み
		cell_id = 'B' + str(sheet_count)
		sheet[cell_id] = average
		sheet_count += 1

	wb.save('SVMResult.xlsx')
	"""
					
	"""
	# 機能の個数が書かれたファイルを呼び出す
	with open('summary_count.csv') as f:
		reader = csv.DictReader(f)
		l = [row for row in reader]
	pprint.pprint(l)
	
	t = time.time() - start
	hour = t / 2400
	minute = (t - hour*2400) / 60
	second = (t - hour*2400 - minute*60)
	print(hour + '時間' + minute + '分' + second + '秒')
	"""
#=================================================================================================
